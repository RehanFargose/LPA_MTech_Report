The use of AI-ML in the Legal field has attracted considerable attention. Since the rise of NLP and LLMs it has become feasible to process large amounts of data and produce results in line with Verdict predictions, Summarization, Precedent finding. There is a large amount of legal data from the last century that has been scanned and digitized which has led to the rise of a Sub-field in the NLP domain known as LJP (Legal Judgement Prediction) \cite{b1} to make the judiciary more efficient and faster. PLMs such as LegalBERT tend to outperform with 88.3 MaF compared to less than 80.2 MaF for generic LLMs and HANs. For documents exceeding 512 tokens, the Longformer-based Lawformer achieved superior results in criminal cases (95.4 MaF for charges) by capturing long-distance dependencies. Integrated frameworks like LADAN + MPBFN reach 96.60% accuracy for article prediction and 96.42% for charge prediction. Despite the 43 datasets and 16 evaluation metrics used, there is still need for a Legal NLP model that is trained primarily on Indian Legal corpus as all existing models are trained on EU, US or Chinese corpora. Black box nature of the output leads to low interpretability which is contrary to the justification based legal landscape. Prison term prediction remains suboptimal (e.g., 42.55 MaR in few-shot settings) due to data distribution challenges. Out of 36 official global languages, support is missing for 27 of them. Only uses judgement summaries and not raw evidence.


A Hybrid 2 stage model that includes RobERTa+DGCNN(stage 1) and T5 PEGASUS (stage 2) was used to create summary of Legal news\cite{b2}. The 1st stage is used to extract sentences and given a vector representation that contains critical information by using the Stage 1 models in conjunction with Average Pooling layer for rapid text vectorisation. DGCNN takes the dimensionality reduced vector representation of extracted sentences. The 2nd layer model(T5) takes the encoded vector, passes them through a Dense layer to produce a single embedding vector (short summary). ROUGE (Recall-Oriented Understudy for Gisting Evaluation) was used as the evaluation measure. The model was found to be effective in summarizing relatively short Legal articles, it displayed a limited ability extract deep/full context (arguments, entities) from longer/complex inputs.





A circumstance aware LJP framework known as NeurJudge was proposed to assist judicial decision making by separating crime facts into adjudicating, statutory and discretionary circumstances to better model what decisions are suitable\cite{b3}. The system introduces a novel approach called Circumstances of Crime aware Fact Separation (CCFS) to extract the facts from the input. An improved model NeurJudge+ utilises graph-based embeddings to distinguish articles/charges that are similar/intertwined. The summaries produced by the model are easy to interpret. The high computational cost coupled with the dataset’s reliance only on Chinese legal corpora make it difficult to directly transfer to Indian Legal landscape. The model struggles with verdict and sentencing when 2 articles are applicable which have high descriptive similarity. Due to the Black box nature it lacks explainability/ interpretability.


A comparison between Word2vec and BERT models was performed while using UDA (Unsupervised Data Augmentation); combining labelled and un-labelled data to increase robustness \cite{b4}. The dataset is scarce and was sourced from the Brazilian Prosecutor’s office’s records, leading to overfitting in BERT. After implementing UDA for Data Augmentation, the Accuracy of both models jumped from 80.7\% to 92\%. The main drawback is the use of Synthetic dataset to augment the model; the resulting performance may not fully generalize to real-world legal scenarios. Small claims court have verbose case descriptions which are constrained heavily by the 512 token limit of BERT models.


BART, Random Forest and LIME(XAI) are used in conjunction with each other to help provide both Summarization, IPC Prediction and Verdict Prediction respectively for the Document. A summary of 150 tokens is generated from a document of max length 1024 tokens\cite{b5} resulting in an accuracy of ~97\%. LIME is incorporated to improve explainability and transparency. However, due to the use of both BART and LIME complexity and computational overhead increases. Using max length of 1024 tokens consumes large amounts of VRAM, whereas limiting to 512 tokens provides comparable performance.

An Ontology-driven knowledge-block summarization method for Chinese judgment document classification was proposed. Domain ontologies and top-level legal ontologies are merged to extract three core blocks: objective facts, subjective intent, and judgment results. The system\cite{b6} uses Word2Vec embeddings, JieBa tokenizer (for Mandarin only) and Word Mover’s Distance (WMD) to compute similarities between extracted blocks, followed by a KNN classifier. Using specific blocks and not entire documents increases both accuracy and speed. However, it requires high quality Ontologies and is linguistically dependent on Chinese corpora only, limiting its transferability between jurisdictions such as India. WMD requires high computational overhead standard models such as Bag of Words, TF-IDF fail to capture document structure and legal semantics in depth.
In line with the UN’s Sustainable Development Goals (SDGs) an ensemble of SVM, Naïve Bayes and LSTM was used to create a system that can correctly label/classify court cases based on their type and what SDG they fall under\cite{b7}. Data augmentation and ensemble strategies were implemented to handle label imbalance between classes, however lack of data from the Brazilian Supreme Court, led to overfitting. All metrics such as Accuracy and F1 score peaked at a stable ~0.80. This model performs effectively when important keywords are present but cannot infer deeper contextual relationships due to the small and restrictive nature of the dataset. SDG model relies heavily on case law/precedents leading to higher complexity and low reliability on cases that do not match any precedents while also being limited to one language (Portuguese).



A text-importance similarity matching framework was proposed to improve long document legal case retrieval. A novel Unsupervised clustering and Contrastive learning approach that identified and preserved only the most critical and factual sentences was created\cite{b8}. These extracted facts were then fed into a BERT based encoder to find Similarity score between Legal documents. Cluster-center distance is used to quantify the impact of each extracted fact; allowing the system to surpass the 512 token limit of the BERT based models. Integrating triplet based contrastive learning and center loss to better differentiate between cases, a final accuracy of 75.08\% was achieved. The system works effectively on larger inputs but is restricted to Chinese Legal corpora and non-transferrable due to differences in Jurisdictions and Language as well as the traditional 512 token limit of BERT models, that cannot be overcome in Indian Legal corpus, unless Contrastive learning is applied. Lower quality of the initial unsupervised clusters can cascade into low accuracy, increase inference latency and event sequence disruption.


  
A Transformer based ECHR case classification framework was proposed to automate the detection of Human rights violations from extremely large Court judgements. A Sliding-window text sequence expansion technique is used to exceed the 512 token limit for BERT based models such as RoBERTa, Legal-BERT, BigBird, ELECTRA. RoBERTa performed the best in the Binary violation classification(F1 score of 86.7\%), whereas for multi-class classification BigBird outperformed all other models with an F1 score of 78.1\% \cite{b9}.Although, the Sliding window approach allows for the BERT based models to exceed their token limits and process larger documents, it incurs high computational overhead and is overdependent on English corpus with good metadata. Adding extra case features such as court branch, importance score leads to diminishing returns due to the text content dominating the feature space. DAPT on LegalBERT and BigBird remains as a point of improvement(unexplored here).


A large scale Bangla NLP Legal corpus named KUMono was created by Web scraping 1.3 million articles across 18 different categories. It contains 353 million word tokens and 1.68 million unique tokens to address the pressing need for a Bangla language corpus. This corpus was further enhanced by using TF-IDF for Article categorization. 6 NLP/ML models were utilised to classify the Court cases present; highest accuracy was achieved by Random Forest and Decision Tree Classifiers with performance metrics exceeding 0.98(Precision, Recall and F1 score) \cite{b10}. KUMono has a large scale, but it lacks context and depth due to dependence on web scraping. Transformer models such as BERT are superior for the purpose of summarization/comparison. The system is also limited to the Bangla corpus with minimal Arabic coverage. Dataset size is low despite Bangla being the 7th most spoken language worldwide.




An SCM (Similar Case Matching) system was developed to enhance long document parsing and similarity matching using a fine-tuned LegalBERT encoder combined with a Dual attention architecture. Local self-attention was used to extra important intra-sentence features, Global attention was used to extract broader context between multiple documents\cite{b11}. The dual attention mechanism allowed the system to outperform existing systems on Cosine, Manhattan and Jaccard metrics. Trained on CAIL+SCM datasets the system was found to have good recall and an accuracy of 89.5\% for Criminal cases and 90.2\% for Civil cases. The dual attention architecture and complex fine-tuning of LegalBERT(12 layers) led to significant computational overhead. The system is limited to Chinese corpora only. The network model lacks semantic depth interaction for Siamese. Usage of basic word frequency model leads to failure in capturing legal jargon and local key features in larger documents.	


Legal NLP is classified into 3 main categories/tasks: Legal Search (retrieval, entailment, QA), Legal Document Review (NER, similarity, classification, summarization), and Legal Prediction—showing that domain-specific models like LEGAL-BERT, LamBERTa, BureauBERTo, ConfliBERT consistently outperform general LLMs such as ChatGPT\cite{b13}. Domain Adaptive Pre-Training (DAPT) on relevant Legal corpora allows smaller NLP models to outperform LLMs and achieve competitive performance for the above 3 tasks with an F1 gain of 7.2\%.  DAPT on legal dataset provides an F1 gain ranging from 15.4\% to 18.2\% as compared to DAPT on generic dataset. The computational cost of specialized NLP models is lower than LLMs but are found to be inferior in long context/large input documents, generalization across jurisdictions (Indian, Chinese, EU, USA, etc) and dataset diversity/size.
A cross-domain LJP frameworks named JurisCTC was proposed to overcome data scarcity in Criminal law by transferring knowledge from Civil law datasets using Unsupervised Data Augmentation (UDA) and Contrastive learning \cite{b14}. A BERT encoder is combined with a class and domain classifier through a Gradient Reversal layer to optimize Maximum Mean Discrepancy (MMD). The system achieves a substantial accuracy of 76.59\% on Criminal law alongside a 78.83\% on Civil law by learning domain invariant representations, The system has very strong generalisation due to UDA, but it is limited only to Chinese Legal corpora and incurs high computational cost for adversarial BERT training. JurisCTC has a higher rate of false positives in the context of criminal cases and trails behind GPT4.0 (75.92\% vs. 83.00\%) for the same. It also requires substantial manual intervention for feature engineering.



Keynote highlights the rapid progress and evolution in NLP, explaining the range of subtasks from basic pre-processing to NER, text similarity, QA, summarization, sliding sequence window, etc. A notable example is the Multi-lingual Legal NLP model developed for Swiss Federal court that can handle 20 different Languages\cite{b15}. Domain pre-trained NLP models such as BERT can provide performance equivalent or surpassing general LLMs with exponentially higher compute power, provided the 5 components: architecture, hyperparameters, training data, model weights/checkpoints, and source code are kept fully open Source. Private firms have an advantage when it comes to powerful models that can handle multiple legal case types, the model in question here required an investment of \$30 million which is infeasible for individuals or smaller teams.



An LJP system named KEMCAN was proposed, utilising a multi-cross attention architecture. The system incorporates legal charge knowledge (definitions, subjective/objective elements, etc) with the fact description to better differentiate between the similar charges/penal codes mentioned in the text\cite{b16}. The system encodes both fact sentences and knowledge units using Bi-GRU + Attention mechanism, mapping each sentence with relevant legal information. The system was able to outperform models such as NeurJudge\cite{b3}, LADAN, BERT-Crime, etc, with a F1 score difference between +3.22\% to +6.5\% over these models. KEMCAN is effective at understanding context but requires a manually refined dataset while also being limited to Chinese criminal legal corpus. KEMCAN was focused primarily on applicable articles and charges, while ignoring the critical subtasks such as prison sentence.


LASG is a legal document summarization framework that was proposed to streamline judicial document analysis by incorporating CKIP transformers (Chinese BERT for sentence embedding) with a PageRank based re-ranking algorithm to extract most representative/important sentences from the documents \cite{b17}. Semantic similarity of extracted facts is computed via cosine similarity after which PageRank is applied to select the top/k-most important sentences to be part of the summary. LASG outperforms BERTSUM and vanilla CKIP transformers and achieves high performance metrics on ROGUE2 (12.72), ROUGE-L (18.33), etc. The system is efficient, lightweight and easy to deploy but is dependent on CKIP transformer. The summaries generated might not encompass the full depth of the corpus. The model lacks domain-specific customization/fine-tuning for different legal case types leading to lack of contextual depth due to being trained primarily on criminal cases. Hallucinations are also a major setback due to the abstractive LLMs.



A legal text classifier was developed to classify petitions for Brazil’s Public Prosecutor’s Office. This study compared TF-IDF, Word2Vec, SVM, Logistic Regression, Decision trees, CNNs, RNNs, and it was found that Word2Vec combined with LSTM encoder achieved the highest performance at 90.47\% Accuracy and F1 score of 85.49\%\cite{b18}, across 18 legal classes and 922,000 cases. This approach offered better semantic generalization relative to traditional bag-of-words models. However, TF-IDF was found to be more effective for simpler classifier models and smaller, more domain-specific datasets, albeit with limited context capacity than BERT based models. Random Under-Sampling (RUS), Over-Sampling were restricted due to computational constraints. 





A systematic study of all NLP/LLM systems found that domain-specific transformer-based NLP models such as BERT can outperform general purpose LLMs while also requiring substantially lower computation resources\cite{b19}. By using DAPT\cite{b13}, Contrastive learning\cite{b8} \cite{b14}, Dual attention architectures\cite{b11}. the F1 score of specialised NLP models such as Legal-BERT can be increased by 8-15\% over a generic LLM. Using techniques such as Sliding sequence window, the 512 token limitation\cite{b9} of traditional BERT models can also be overcome. The models also require high cost expert annotated judgements for reference.


LegalRAG is a RAG (Retrieval Augmented Generation) based framework designed for low-resource legal documents for the Bangla corpus. It compares and utilizes Llama3.2(3B) and Llama3.1(8B) wherein the cosine similarity increases from 0.76 to 0.82 when transferring from the former to the latter\cite{b20}. The dataset is augmented by using RAG to add relevant data scraped from external web sources. Due to the scarcity of the Bangla corpus, synthetic data was used to augment the overall dataset. The system has high accuracy for Bangla/English corpus but is constrained due to the low dataset size and unavailability of relevant data to scrape. The system exhibits overfitting lack of DAPT and overall low resource nature of the dataset. The usage of synthetic data leads to poor out of context vulnerability, closed loop bias and computational latency trade-offs.


Pre-trained Language Models (PLMs) across 8 legal datasets were evaluated and it was observed that they outperformed non-PLM models by 4\%-35\% on most NLP tasks. Domain specific models such as LegalBERT were the only models that could surpass the performance of PLMs\cite{b21}, achieving marginal performance gains of 2\%-5\%. PLMs demonstrated strengths in handling legal terminology, complex reasoning and better recall for multi-label tasks. However, they underperformed by 5\% or more in regards to cross-domain transferability and limited to a 512 token length.  Domain specific PLMs also exhibited limited transferability between different legal sub-domains. Diminishing returns in F1 score were exhibited when processing larger legal documents; 1.5\% gain in F1 score required 7 times longer training. PLM retrieval suffered from low accuracy due to difficulty in handling shared keywords that are legally irrelevant which lead to a gap in legal semantic matching.


HANOI-Legal is a parallel learning framework that adapts Pre-trained Language Models (PLMs) using Uniprompt; a unified QA style prompting scheme that reformulates diverse datasets into a single text-to-text format. Built on an encoder-decoder PLM(Randeng-T5-784M) \cite{b22}, the system performs unified prompt-based fine tuning yielding strong gains; +22.13\% F1 on CivilEE-CLS dataset and +46.35\% and +41.46\% on CivilEE-Args and CJRC datasets, respectively. However, the performance of the system is constrained by the relatively small size of the T5 model. HANOI performs best in data-scarce environments only, in resource rich environments other models surpass it. HANOI framework’s scalability for larger models (100B+ parameters) is unpredictable. 





An NLP model was created to predict the outcomes of Philippines SC corpora. The system incorporated bag of words n-grams with spectral clustering-based classifiers alongside popular classifiers such as SVM and Random Forest. The dataset was small and included approximately 6,500 cleaned and metadata tagged SC cases. SVM with n-grams had an accuracy of 45\%; improved to 55\% with topic-cluster features\cite{b23}. The best performance was provided by Random Forest classifier with topic-cluster features at 59\%. The models were simple and computationally light, but due to the small dataset and lack of standardized legal document format significantly hindered the performance of the models. Bag of words model is insufficient for extracting abstract legal reasoning due to courts focusing on “questions of law” rather than “questions of facts”.





An NLP summarization model was created for the Turkish Constitutional Court decisions that utilised an expertly annotated 1300 case dataset fed to a BERT2BERT model to produce summaries and verdict prediction was performed using XGBoost. The extractive-abstractive nature of the models enabled it to circumvent the 512 token limit of BERT. The XGBoost model was able to attain a 93.84\%\cite{b24} accuracy when fed full texts and 62.30\% accuracy when fed BERT generated summaries. The main advantage of this Hybrid approach was high accuracy of prediction and summarization with relatively low computational cost in part due to the smaller dataset. However, due to the small dataset and its need to be annotated by experts, scalability is challenging due to the computational overhead of BERT2BERT. The model is limited to Constitutional court, and does not generalize well for Criminal and Administrative laws. The models lack transparency and there is a need to introduce XAI to improve interpretability.



Transformer based models (BERT) were compared with LLMs (Llama) to evaluate their effectiveness for summarizing Portuguese legal documents. A highly annotated and expertly curated dataset of 2,373 documents was used as the evaluation base. LegalBERT and BERT-TRJ were compared with Llama3.1(70B) and Gemma2(27B) for the NER task. The fine-tuned BERT models had the highest F1 score lying between 0.74-0.96\cite{b25}; outperforming LLMs due to their higher token-level precision. Llama3.1 was tested in a zero-shot method and achieved a peak F1 score of 0.93. Due to the imbalance in dataset and small size; generalization was limited. The LLMs could not handle complex, multi-span legal entities. Confidentiality constraints surrounding source documents and expert annotations led to issues with reproducibility. Gemma2 extracted excessive amounts of irrelevant information and also suffered from hallucinations.
