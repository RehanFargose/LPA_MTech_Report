\chapter{Proposed Methodology}
\label{ch:methodology}

\section{Overview}
This chapter outlines the proposed development methodology, implementation phases, testing procedures, and evaluation metrics for the VR driving simulator. The methodology follows an iterative design approach with continuous validation against the functional and non-functional requirements specified in Chapter~\ref{ch:system_analysis}.
\section{Development Phases}

\subsection{Phase 1: Data Collection and Preprocessing (Weeks 1--4)}
\textbf{Objective:} Acquire, clean, and structure legal documents required for downstream analysis.

\textbf{Tasks:}
\begin{itemize}
	\item Collect publicly available trial court and High Court judgments relevant to family law matters
	\item Organize documents into structured storage (PDFs, metadata JSON, parquet indexes)
	\item Extract raw text from PDFs using Fitz (PyMuPDF)
	\item Perform text cleaning: noise removal, normalization, OCR artifact correction
	\item Segment documents into logical sections (facts, arguments, evidence, judgment)
	\item Assign unique case identifiers and metadata tags (court, year, case type)
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
	\item Curated dataset of court judgments and supporting documents
	\item Automated PDF-to-text preprocessing pipeline
	\item Structured and cleaned legal text corpus
	\item Data documentation and schema description
\end{itemize}

\subsection{Phase 2: Legal Entity Recognition and IPC Mapping (Weeks 5--8)}
\textbf{Objective:} Identify legal entities and map relevant statutory provisions.

\textbf{Tasks:}
\begin{itemize}
	\item Fine-tune Legal-BERT for named entity recognition on Indian legal text
	\item Extract entities such as parties, dates, courts, IPC sections, and legal acts
	\item Implement IPC and section classification module using supervised learning
	\item Validate extraction accuracy against manually annotated samples
	\item Store extracted entities and mapped sections in structured form
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
	\item Trained Legal-BERT NER model
	\item IPC/section mapping module
	\item Evaluation report (precision, recall, F1-score)
	\item Annotated validation dataset
\end{itemize}

\subsection{Phase 3: Evidence Scrutinization and Truth Scoring (Weeks 9--12)}
\textbf{Objective:} Analyze evidence consistency and assign interpretive truth scores.

\textbf{Tasks:}
\begin{itemize}
	\item Identify and align evidence statements from both parties
	\item Implement contradiction detection using LLaMA 3 on segmented evidence blocks
	\item Design heuristic-assisted truth scoring mechanism based on contradiction severity
	\item Link evidence blocks to corresponding legal claims and sections
	\item Generate explainable intermediate outputs for transparency
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
	\item Evidence alignment and contradiction detection module
	\item Truth score computation framework
	\item Case-wise structured evidence representation
	\item Qualitative validation on selected cases
\end{itemize}

\subsection{Phase 4: Precedent Retrieval System (Weeks 13--16)}
\textbf{Objective:} Retrieve legally similar past cases using semantic similarity.

\textbf{Tasks:}
\begin{itemize}
	\item Generate contextual embeddings using Legal-BERT
	\item Construct vector database of precedent judgments
	\item Implement Similar Case Matching (SCM) using approximate nearest neighbor search
	\item Rank retrieved precedents based on factual similarity and applied IPC sections
	\item Validate retrieval relevance using expert-reviewed samples
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
	\item Precedent embedding index
	\item SCM-based retrieval engine
	\item Ranked precedent lists for test cases
	\item Retrieval performance analysis
\end{itemize}

\subsection{Phase 5: Verdict Prediction and Summarization (Weeks 17--20)}
\textbf{Objective:} Predict likely verdicts and generate concise explanations.

\textbf{Tasks:}
\begin{itemize}
	\item Integrate case facts, truth scores, IPC mappings, and precedents
	\item Implement verdict and sentence prediction using LLaMA 3
	\item Design prompt templates to minimize hallucination and ensure consistency
	\item Generate simplified summaries under 150 words
	\item Validate outputs against known case outcomes
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
	\item Verdict and sentence prediction module
	\item Simplified explanation generator
	\item Case-wise prediction outputs
	\item Error analysis and refinement report
\end{itemize}

\subsection{Phase 6: Evaluation, Refinement, and Documentation (Weeks 21--24)}
\textbf{Objective:} Evaluate system performance and finalize thesis-ready outputs.

\textbf{Tasks:}
\begin{itemize}
	\item Conduct quantitative evaluation (accuracy, precision, recall)
	\item Perform qualitative case studies and expert review
	\item Analyze model bias, failure cases, and limitations
	\item Optimize pipeline efficiency and modularity
	\item Prepare final documentation, diagrams, and thesis chapters
\end{itemize}

\textbf{Deliverables:}
\begin{itemize}
	\item Comprehensive evaluation report
	\item Refined end-to-end system pipeline
	\item Final thesis documentation and figures
	\item Deployment-ready prototype (academic)
\end{itemize}




\newpage
\section{Evaluation Metrics}
\subsection{Technical Performance Metrics}
\begin{itemize}
	\item \textbf{Text Extraction Accuracy:} Percentage of correctly extracted and readable text from judicial PDFs. Target: $>$ 95\% extraction success on digitally available judgments.
	\item \textbf{Named Entity Recognition (NER) Performance:} Precision, recall, and F1-score for legal entities (parties, dates, IPC sections). Target: F1-score $>$ 0.85 on annotated validation set.
	\item \textbf{IPC / Section Mapping Accuracy:} Correct identification of applicable statutory provisions compared to ground truth judgments. Target: Accuracy $>$ 80\%.
	\item \textbf{Precedent Retrieval Quality:} Top-$k$ retrieval relevance measured using Recall@$k$ (e.g., $k=5$). Target: Recall@5 $>$ 70\%.
	\item \textbf{Inference Efficiency:} Average processing time per case for the full pipeline (excluding offline preprocessing). Target: $<$ 2 minutes per case in batch mode.
\end{itemize}

\subsection{Reasoning and Prediction Metrics}
\begin{itemize}
	\item \textbf{Verdict Prediction Accuracy:} Percentage of cases where predicted verdict matches actual outcome. Target: Accuracy $>$ 75\%.
	\item \textbf{Sentence / Relief Prediction Error:} Deviation between predicted and actual sentencing outcomes or relief granted (where applicable). Target: Qualitative alignment in $>$ 70\% of evaluated cases.
	\item \textbf{Evidence Consistency Score Validity:} Correlation between system-generated truth scores and expert-assessed evidence reliability. Target: Positive correlation ($r > 0.6$).
	\item \textbf{Explainability Coverage:} Proportion of predictions accompanied by identifiable reasoning elements (mapped sections, precedents, evidence references). Target: 100\%.
\end{itemize}

\subsection{Usability and Practical Relevance Metrics}
\begin{itemize}
	\item \textbf{User Satisfaction:} Likert-scale ratings (1--5) from law students or legal researchers on usefulness and clarity. Target: Mean score $>$ 4.0.
	\item \textbf{Interpretability Score:} User-reported ease of understanding system outputs (verdict reasoning, summaries). Target: Mean score $>$ 4.0.
	\item \textbf{Adoption Potential:} Percentage of users indicating willingness to reuse the system for legal research tasks. Target: $>$ 70\%.
\end{itemize}

\subsection{Extended Evaluation Metrics (Future Work)}
\begin{itemize}
	\item \textbf{Cross-Domain Generalization:} Performance stability when extending beyond family law to other legal domains.
	\item \textbf{Bias Assessment:} Statistical analysis of prediction disparities across case types, parties, or outcomes.
	\item \textbf{Human-in-the-Loop Validation:} Comparative evaluation of system-assisted vs manual legal research efficiency.
\end{itemize}



\newpage
\section{Testing Procedures}
\subsection{Data and Preprocessing Validation Tests}
\begin{enumerate}
	\item \textbf{Text Extraction Accuracy:} Randomly sample extracted PDF text and manually compare with source documents to verify completeness and readability.
	\item \textbf{Segmentation Validation:} Verify correct separation of facts, arguments, evidence, and judgments using annotated samples.
	\item \textbf{Metadata Consistency Check:} Validate correctness of court, year, and case-type metadata across stored records.
\end{enumerate}

\subsection{Model-Level Validation Tests}
\begin{enumerate}
	\item \textbf{NER Accuracy Test:} Evaluate Legal-BERT NER output against manually annotated ground truth using precision, recall, and F1-score.
	\item \textbf{IPC Mapping Verification:} Compare predicted IPC/section mappings with sections cited in final judgments.
	\item \textbf{Contradiction Detection Test:} Manually inspect detected contradictions in selected cases to assess logical correctness.
	\item \textbf{Precedent Retrieval Test:} Evaluate relevance of top-$k$ retrieved precedents through expert judgment.
\end{enumerate}

\subsection{End-to-End System Validation}
\begin{enumerate}
	\item \textbf{Verdict Prediction Evaluation:} Compare predicted verdicts with actual case outcomes across a held-out test set.
	\item \textbf{Reasoning Consistency Check:} Verify alignment between predicted verdict, cited precedents, and supporting evidence.
	\item \textbf{Summarization Quality Test:} Assess summaries for factual correctness, completeness, and adherence to word limit.
\end{enumerate}

\subsection{User Evaluation Protocol}
\begin{enumerate}
	\item \textbf{Pre-Evaluation:} Brief users (law students/researchers) on system scope and non-advisory nature.
	\item \textbf{Task-Based Testing:} Users analyze selected cases using the system and manually, under time constraints.
	\item \textbf{Post-Evaluation:} Collect Likert-scale feedback on usefulness, interpretability, and trustworthiness.
	\item \textbf{Data Collection:} Log task completion time, user feedback, and qualitative observations.
\end{enumerate}



\newpage
\section{Risk Mitigation}

\subsection{Technical Risks}
\begin{itemize}
	\item \textbf{Risk:} Poor text extraction quality from scanned or low-quality PDFs
	\textbf{Mitigation:} Restrict dataset to digitally generated judgments; apply OCR correction and rule-based cleaning; manual validation on samples
	\item \textbf{Risk:} NER and IPC mapping inaccuracies propagate downstream
	\textbf{Mitigation:} Use confidence thresholds; cross-validate with rule-based section detection; allow manual correction for evaluation
	\item \textbf{Risk:} High computational cost of LLaMA 3 inference
	\textbf{Mitigation:} Process segmented text blocks; batch inference; limit token length; offline execution
\end{itemize}

\subsection{Reasoning and Model Risks}
\begin{itemize}
	\item \textbf{Risk:} Hallucinated or unsupported verdict predictions
	\textbf{Mitigation:} Constrain prompts to extracted facts and mapped sections; require precedent citation; include non-advisory disclaimers
	\item \textbf{Risk:} Contradiction detection yields false positives
	\textbf{Mitigation:} Combine LLM outputs with heuristic checks; manual review for evaluation cases; threshold-based truth scoring
\end{itemize}

\subsection{Development and Evaluation Risks}
\begin{itemize}
	\item \textbf{Risk:} Limited availability of labeled ground truth data for Famil courts
	\textbf{Mitigation:} Use mixed evaluation (quantitative + qualitative); expert-reviewed case studies from Law textbooks
	\item \textbf{Risk:} Schedule overruns due to integration complexity
	\textbf{Mitigation:} Modular pipeline design; incremental testing; buffer time in final phase
\end{itemize}

\newpage
\section{Expected Outcomes}
Upon completion of the proposed methodology, the project is expected to deliver:
\begin{enumerate}
	\item A functional Legal Precedent Assistant capable of analyzing Indian Family court case documents
	\item An end-to-end NLP pipeline integrating text extraction, evidence analysis, precedent retrieval, and verdict prediction
	\item Structured outputs including mapped IPC sections, retrieved precedents, and simplified case summaries
	\item Evaluation results demonstrating technical performance and predictive accuracy on real judicial data
	\item Identified limitations, biases, and areas for methodological improvement
	\item A scalable research foundation for future extensions to additional legal domains and languages
\end{enumerate}

The validation results will support iterative refinement of the system and demonstrate the feasibility of AI-assisted legal research as a decision-support tool for academic and professional use.

